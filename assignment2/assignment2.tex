\documentclass[10pt]{article}
\usepackage{fullpage,enumitem,amsmath,amssymb,graphicx}
\usepackage{tikz}
\usepackage{verbatim}

\begin{document}

\begin{center}
{\Large CS224N Winter 2016 Homework [2]}

\begin{tabular}{rl}
SUNet ID: & [06074217] \\
Name: & [Jiajun Sun] \\
\end{tabular}
\end{center}

By turning in this assignment, I agree by the Stanford honor code and declare
that all of this is my own work.

\section*{Problem 1}
\begin{enumerate}[label=(\alph*)]
\item
\item
\item
The placeholder variable are created to store input data. There are two placeholder variable one for $X$ the other for label.
Feed dictionaries are used for specifying variable for tensorflow placeholder.
Here feed dictionaries feed batch data to two placeholders that we created.
\item
\item
Tensorflow utilises the computation graph user created and automatically computes the gradient against all the tensorflow variable.

\end{enumerate}

\pagebreak[4]
\section*{Problem 2}
\begin{enumerate}[label=(\alph*)]
\item
Below table shows the sequence of transitions needed for parsing the sentence.
\begin{center}
\begin{tabular}{ l | l | l | l}
  stack & buffer & new dependency & transition\\ \hline
  \big[ROOT\big] & \big[I, parsed, this, sentence, correctly\big] &  & Initial Configuration\\
  \big[ROOT, I\big] & \big[parsed, this, sentence, correctly\big] &  & SHIFT\\
  \big[ROOT, I, parsed\big] & \big[this, sentence, correctly\big] &  & SHIFT\\
  \big[ROOT, parsed\big] & \big[this, sentence, correctly\big] & parsed $\rightarrow$ I & LEFT-ARC\\
  \big[ROOT, parsed, this\big] & \big[sentence, correctly\big] & & SHIFT\\
  \big[ROOT, parsed, this, sentence\big] & \big[correctly\big] & & SHIFT\\
  \big[ROOT, parsed, sentence\big] & \big[correctly\big] & sentence $\rightarrow$ this & LEFT-ARC\\
  \big[ROOT, parsed\big] & \big[correctly\big] & parsed $\rightarrow$ sentence & RIGHT-ARC\\
  \big[ROOT, parsed, correctly\big] & \big[ \big] & & SHIFT\\
  \big[ROOT, parsed\big] & \big[ \big] & parsed $\rightarrow$ correctly & RIGHT-ARC\\
  \big[ROOT\big] & \big[ \big] & ROOT $\rightarrow$ parsed & RIGHT-ARC\\
\end{tabular}
\end{center}
\item
A sentence contains $n$ words will be parsed in $2n$ steps.
Within these $2n$ steps, $n$ steps are used for SHIFT and the other $n$ steps are used for LEFT-ARC or RIGHT-ARC.

\item
\item
\item

\item
First let`s derive the expression for the expectation:
\begin{equation*}
	\mathbb{E}_{p_{drop}}[h_{drop}]_i = \gamma(1-p_{drop})h_i = h_i
\end{equation*}
Therefore, in order for the equation above to hold:
\begin{equation}
	\gamma = \frac{1}{1-p_{drop}}
\end{equation}

\item
The $m$ step will merge the immediate updates with previous updates.
For example if $\beta_1 = 0.9$, the immediate has a weight of $0.1$ and previous has a weight of $0.9$.
This step help the training to be easier to converge.
Normal SGD process usually carries stochastic fluctuation at the end of training.\\
\\
$\sqrt{\nu}$ is actually a scaling factor for learning rate.
When gradient is larger, the learning rate will be scaled down and vice versa.
This process can give the training faster converge compared to fix learning rate.

\item
best UAV on dev sets: 88.50\\
best UAV on test sets: 89.12\\
\end{enumerate}


\pagebreak[4]
\section*{Problem 3}
\begin{enumerate}[label=(\alph*)]
\item
Since $y_j^{t}$ is a one-hot vector, let`s assume its $m$ element is $1$, which is $y_{j,m}^{t} = 1$.
Therefore the perplexity and cross-entropy loss becomes:
\begin{equation*}
	J_{\theta}^{t} = - \log{\hat{y}_{j,m}^{t}}
\end{equation*}
\begin{equation*}
	PP^{t}(y^t, \hat{y^t}) = \frac{1}{\hat{y}_{j,m}^{t}}
\end{equation*}
When the cross-entropy loss gets its minimal, $\hat{y}_{j,m}^{t}$ gets its maximal at the same time.
As a result, perplexity gets its minimal as well.\\
\\
When the prediction is random (uniform distribution):
\begin{equation}
	\hat{y}_{j}^{t} = [1/|V|, 1/|V|, ..., 1/|V|, 1/|V|]
\end{equation}
Therefore, the cross-entropy loss becomes when $|V|=10000$:
\begin{equation}
	J_{\theta}^{t} = -\log{1/10000} = \log{10000}
\end{equation}
and the perplexity:
\begin{equation}
	PP^{t}(y^t, \hat{y^t}) = \frac{1}{1/10000} = 10000
\end{equation}

\item
Given we have derive the gradient for cross-entropy and softmax function in previous assignment, below are some properties we already know:
\begin{equation}
	\sigma' = \sigma(1-\sigma)
\end{equation}

\begin{equation}
	\frac{\partial CE(y, \hat{y})}{\partial softmax(\theta)}\frac{softmax(\theta)}{\partial \theta} = (\hat{y} - y)
\end{equation}

In the below derivation, I will directly apply above equations:
\begin{equation}
\begin{aligned}
	\frac{\partial J^{(t)}}{\partial b_2} & = \frac{\partial J^{(t)}}{\partial \hat{y}^{(t)}}
	\frac{\partial \hat{y}^{(t)}}{\partial b_2}\\
	& = \sum_i\frac{\partial J^{(t)}}{\partial \hat{y_i}^{(t)}}
	\frac{\partial \hat{y_i}^{(t)}}{\partial b_{2,j}}
	= \sum_i\frac{-y_i^{(t)}}{\hat{y_i}^{(t)}}
	\frac{\partial \hat{y_i}^{(t)}}{\partial b_{2,j}}\\
	& = \sum_i\frac{-y_i^{(t)}}{\hat{y_i}^{(t)}}\hat{y_i}^{(t)}(y_j^{(t)}-\hat{y_j}^{(t)})\mathbf{1}\\
	& = -(y_j^{(t)}-\hat{y_j}^{(t)})\mathbf{1}\\
	& = \hat{\mathbf{y}}^{(t)} - \mathbf{y}^{(t)}
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
	\frac{\partial J^{(t)}}{\partial L_x^{(t)}} & = \frac{\partial J^{(t)}}{\partial \hat{y}^{(t)}}
	\frac{\partial \hat{y}^{(t)}}{\partial h^{(t)}}\frac{\partial h^{(t)}}{\partial e^{(t)}}\frac{\partial e^{(t)}}{\partial L_x^{(t)}}\\
	& = \sum_i(\frac{\partial J^{(t)}}{\partial \hat{y}^{(t)}}\frac{\partial \hat{y}^{(t)}}{\partial h^{(t)}})_i
	\frac{\partial h_i}{\partial e^{(t)}_j}\frac{\partial e^{(t)}_j}{\partial L_x^{(t)}}\\
	& = \sum_i (\hat{\mathbf{y}}^{(t)} - \mathbf{y}^{(t)})U^{T}_ih^{(t)}_i(1-h^{(t)}_i)I^{T}_{i,j}1\\
	& = (\hat{\mathbf{y}}^{(t)} - \mathbf{y}^{(t)})U^{T}\circ h^{(t)}\circ(1-h^{(t)})I^{T}
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
	\left.\frac{\partial J}{\partial I}\right|_{(t)} & = \frac{\partial J^{(t)}}{\partial \hat{y}^{(t)}}
	\frac{\partial \hat{y}^{(t)}}{\partial h^{(t)}}\frac{\partial h^{(t)}}{\partial I}\\
	& = (\frac{\partial J^{(t)}}{\partial \hat{y}^{(t)}}\frac{\partial \hat{y}^{(t)}}{\partial h^{(t)}})_i\frac{\partial h_i}{\partial I_{i,j}}\\
	& = (\hat{\mathbf{y}}^{(t)} - \mathbf{y}^{(t)})U^{T}_ih^{(t)}_i(1-h^{(t)}_i)e^{(t)}_j\\
	& = e^{(t)^T}(\hat{\mathbf{y}}^{(t)} - \mathbf{y}^{(t)})U^{T}\circ h^{(t)}\circ(1-h^{(t)})
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
	\left.\frac{\partial J}{\partial H}\right|_{(t)} & = \frac{\partial J^{(t)}}{\partial \hat{y}^{(t)}}
	\frac{\partial \hat{y}^{(t)}}{\partial h^{(t)}}\frac{\partial h^{(t)}}{\partial H}\\
	& = (\frac{\partial J^{(t)}}{\partial \hat{y}^{(t)}}\frac{\partial \hat{y}^{(t)}}{\partial h^{(t)}})_i\frac{\partial h_i}{\partial H_{i,j}}\\
	& = (\hat{\mathbf{y}}^{(t)} - \mathbf{y}^{(t)})U^{T}_ih^{(t)}_i(1-h^{(t)}_i)h^{(t-1)}_j\\
	& = h^{(t-1)^T}(\hat{\mathbf{y}}^{(t)} - \mathbf{y}^{(t)})U^{T}\circ h^{(t)}\circ(1-h^{(t)})
\end{aligned}
\end{equation}

\item
First denotes $\frac{\partial J^{(t)}}{\partial h^{(t-1)}}$ as $\delta^{(t-1)}$ and:
\begin{equation*}
	\delta^{(t-1)} =(\hat{\mathbf{y}}^{(t)} - \mathbf{y}^{(t)})U^{T}\circ h^{(t)}\circ(1-h^{(t)})H^{T}
\end{equation*}

\begin{equation}
\begin{aligned}
	\frac{\partial J^{(t)}}{\partial L_x^{(t-1)}} & = \frac{\partial J^{(t)}}{\partial h^{(t-1)}}
	\frac{\partial h^{(t-1)}}{\partial L_x^{(t-1)}}\\
	& = (\delta^{t-1}\circ h^{(t-1)}\circ(1-h^{(t-1)}))I^{T}
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
	\left.\frac{\partial J^{(t)}}{\partial I}\right|_{(t-1)} & = \frac{\partial J^{(t)}}{\partial h^{(t-1)}}
	\frac{\partial h^{(t-1)}}{\partial I}\\
	& = e^{(t-1)^T}(\delta^{t-1}\circ h^{(t-1)}\circ(1-h^{(t-1)}))
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
	\left.\frac{\partial J^{(t)}}{\partial H}\right|_{(t-1)} & = \frac{\partial J^{(t)}}{\partial h^{(t-1)}}
	\frac{\partial h^{(t-1)}}{\partial H}\\
	& = h^{(t-2)^T}(\delta^{t-1}\circ h^{(t-1)}\circ(1-h^{(t-1)}))
\end{aligned}
\end{equation}

\item
\textbf{Forward cost:}\\
For the forward propogation,
\begin{equation*}
	cost(h) = o(D_h^2) + o(D_h)
\end{equation*}
\begin{equation*}
	cost(\sigma) = o(D_h|V|)
\end{equation*}
\begin{equation*}
	cost(J) = o(|V|)
\end{equation*}
Therefore, the total cost for forward propogation:
\begin{equation*}
	o(D_h^2) + o(D_h) + o(D_h|V|) + o(|V|) \approx o(D_h^2) + o(D_h|V|)
\end{equation*}
If assume the vocabulary size $|V|$ is much greater than hidden layer dimension, we can have:
$$
cost \approx o(D_h|V|)
$$


\textbf{Backward cost:}\\
The cost for backward propogation is comprised of the following element:
\begin{equation*}
cost(\delta^{(t-1)}) = o(D_h|V|) + o(D_h) + o(D_h^2)
\end{equation*}

\begin{equation*}
cost(\frac{\partial J^{(t)}}{\partial L_x^{(t-1)}}) = o(D_h) + o(dD_h)
\end{equation*}

\begin{equation*}
cost(\left.\frac{\partial J^{(t)}}{\partial I}\right|_{(t-1)}) = o(dD_h)
\end{equation*}

\begin{equation*}
cost(\left.\frac{\partial J^{(t)}}{\partial H}\right|_{(t-1)}) = o(D_h^2)
\end{equation*}

Therefore, the total cost for backward propogation is:
\begin{equation*}
o(D_h|V|) + o(D_h) + o(D_h^2) + o(D_h) + o(dD_h) + o(dD_h) + o(D_h^2) \approx o(D_h|V|) + o(dD_h) + o(D_h^2)
\end{equation*}
If we assume the vocabulary size is much larger than other dimensions:
$$
cost \approx o(D_h|V|)
$$
When adding multiple steps back in time, the cost matters is for calculating $\delta$:
$$
\begin{aligned}
cost & \approx n\times cost(\delta)\\
&= o(nD_h|V|) + o(nD_h) + o(nD_h^2)\\
& \approx o(D_h|V|)
\end{aligned}
$$

\item
We find most the computation are constriants by the dimension of vocabulary.
One possible way is to add a layer that rather take work vector into a word class vector.
Then take this word class vector into the RNN. This should reduce the cost for both forward and backward propogation.


\end{enumerate}

\end{document}

